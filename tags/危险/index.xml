<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>危险 on 读写错误</title>
    <link>https://ioerr.github.io/tags/%E5%8D%B1%E9%99%A9/</link>
    <description>Recent content in 危险 on 读写错误</description>
    <generator>Hugo -- 0.126.0</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sun, 29 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://ioerr.github.io/tags/%E5%8D%B1%E9%99%A9/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>杞人忧天</title>
      <link>https://ioerr.github.io/posts/qiren-youtian/</link>
      <pubDate>Sun, 29 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://ioerr.github.io/posts/qiren-youtian/</guid>
      <description>ZoomQuiet： AI 安全和以往最大的不同在, 这种安全留给人类反应的时间越来越短, 不像 DDT 时代,我们用20年发现有问题, 用10年修正, 整个儿生</description>
      <content:encoded><![CDATA[<p><a href="https://x.com/zoomq/status/1840237734071607371">ZoomQuiet</a>：<br>
AI 安全和以往最大的不同在,<br>
这种安全留给人类反应的时间越来越短,<br>
不像 DDT 时代,我们用20年发现有问题, 用10年修正,<br>
整个儿生态并没崩溃;<br>
而 AI 的安全问题可能, 几百亳秒后, 超级智能诞生,<br>
因为最初一行道德代码中一个标点错误,<br>
人类就消失了,我们一点儿改善/修订的机会都没有&hellip;</p>
<p><a href="https://x.com/calon/status/1840252252650623154">Calon</a>：<br>
我目前仍然觉得这是杞人忧天。<br>
超级智能诞生，和超级智能能够认识、理解、改造和掌握现实世界，中间还有巨大的鸿沟。</p>
<p>对超级智能的担忧某种程度上是人类对自身能力自负的投射和放大。<br>
虽然人类会认为自己的智能会赶不上超级人工智能，看上去是谦虚、自卑，但实际上也可能是认为自己凭借智能优势已经成为万物之灵、世界的主宰。<br>
如果出现一个比自己更智慧的存在，那可不得了，一定会是新的霸主，而且会无法克服并继承和放大自己的贪婪、残忍、极端…这个新的霸主一定能够凭借智慧突破认知瓶颈，无往而不利，直接飞升成仙都不为过。</p>
<p>人类以为自己在这个境界的临门处徘徊着，只要超越自己就行了。<br>
也许实际上还差得远呢——即使是超越了人类的超级智能，也只是鸭立鸡群的程度而已。</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
